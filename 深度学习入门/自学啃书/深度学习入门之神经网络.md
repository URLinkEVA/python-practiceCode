神经网络的一个重要性质是可以自动地从数据中学习到合适的权重参数
# 从感知机到神经网路
神经网络与上一章的感知机有很多共同点，主要以两者差异为中心，介绍神经网络的结构

## 神经网络的例子
![在这里插入图片描述](https://img-blog.csdnimg.cn/1253355407454ff2ad64a2702f5a956a.png)
左边一列为输入层，最右边一列称为输出层，中间的一列称为中间层（隐藏层）

改写之前的感知数学式
![在这里插入图片描述](https://img-blog.csdnimg.cn/5686a5e234a94820bfad66aff8a1eeb8.png)
引入新函数h(x)
![在这里插入图片描述](https://img-blog.csdnimg.cn/04aa465c00554d13bd845a5bb5d2757e.png)
输入信号的总和会被h(x)转换，转换后的值就是输出y
## 激活函数出场
上面的h(x)函数就是**激活函数**(activation function)
![在这里插入图片描述](https://img-blog.csdnimg.cn/d6a0683da9c74fdcbe6525666e4aead4.png)
信号的加权总和为节点a，然后节点a被激活函数h()转换成节点y。

神经元和节点两个术语含义相同
# 激活函数
上面的激活函数以阈值为界，输入一超过阈值，就切换输出，这样的函数称为“阶跃函数”，神经网络用的是其他的激活函数
## sigmoid函数
经常使用的就是这一个sigmoid函数
![在这里插入图片描述](https://img-blog.csdnimg.cn/85f467cd96fb4a898503df6cb1e831a0.png)
更常见的是这个写法
![在这里插入图片描述](https://img-blog.csdnimg.cn/9bf1c7b2e63d4d44bb470813b07ec916.png)
e是纳皮尔常数2.718281828459045...

## 阶跃函数的实现

```python
def step_func(x):
    if x > 0:
        return 1
    else:
        return 0
```
实现简单，但参数x只接受实数，将其修改为支持numpy数组的实现

```python
import numpy as np
def step_func(x):
    y = x > 0
    return y.astype(np.int)
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/0181f12f76f141e88cf8c0aa87014819.png)

## 阶跃函数的图形

```python
import numpy as np
import matplotlib.pyplot as plt

def step_func(x):
    return np.array(x>0,dtype=np.int)

x = np.arange(-5.0,5.0,0.1)
y = step_func(x)
plt.plot(x,y)
plt.ylim(-0.1,1.1)
plt.show()
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/b800dfcaa28849eaa53a497c89a71957.png)
以0为界，输出从0换到1，值呈阶梯式变化，所以称为阶梯函数
## sigmoid函数的实现

```python
import numpy as np
import matplotlib.pyplot as plt
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.arange(-5.0,5.0,0.1)
y = sigmoid(x)
plt.plot(x,y)   
plt.ylim(-0.1,1.1)
plt.show()
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/d8f99415c294448999a6afea6042d1dc.png)

## sigmoid函数和阶跃函数的比较
![在这里插入图片描述](https://img-blog.csdnimg.cn/5db5716ae46f4a7fbd178d855d7dd3d9.png)
sigmoid是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。sigmoid函数的平滑性对神经网络的学习具有重要意义。
## 非线性函数
sigmoid函数与阶跃函数还有个共同点，就是两者均为**非线性函数**。神经网络的激活函数必须使用非线性函数，因为线性函数进行神经网络的叠层没有意义。
## ReLU函数
最近的激活函数有ReLU(Rectified Linear Unit)函数
![在这里插入图片描述](https://img-blog.csdnimg.cn/3c89aeab87ea4cecade5004896d0ba86.png)
也是一个非常简单的函数

```python
import numpy as np
import matplotlib.pyplot as plt
def relu(x):
    return np.maximum(0,x)

x = np.arange(-5.0,5.0,0.1)  
y = relu(x)
plt.plot(x,y)   
plt.ylim(-0.1,6)
plt.show()
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/5bfd49b882c749159b90b448a05f7b5d.png)
这里使用了numpy的maximum函数。maximum函数会从输入的数值中选择较大的那个值进行输出
