借用华为云实验熟悉操作
# 配置环境
## 创建数据库及数据库表
- 绑定弹性公网IP
- 新建数据库“vmall”
- 新建表，表名“product”，其他参数默认

添加3个字段分别如下：
```
1.列名id，类型int，长度11，勾选主键，扩展信息如下图（id自增长）；
2.列名title，类型varchar，长度255，勾选可空；
3.列名image，类型varchar，长度255，勾选可空。
```
立即创建后弹出SQL预览界面
```
CREATE TABLE `vmall`.`product` (
	`id` INT(11) UNSIGNED NOT NULL AUTO_INCREMENT,
	`title` VARCHAR(255) NULL,
	`image` VARCHAR(255) NULL,
	PRIMARY KEY (`id`)
)	ENGINE = InnoDB
	DEFAULT CHARACTER SET = utf8mb4
	COLLATE = utf8mb4_general_ci;
```
执行脚本


# 查看目的网页并编写爬虫代码
随便打开个网址，例如[华为商城](https://sale.vmall.com/huaweizone.html)

按“F12”查看网页元素，选择“鼠标跟随”按钮查看元素，然后点击网页中某个元素，可以看到源码界面显示了此元素对应的源码片段，从该源码片段中找到元素class或是id属性

## 创建爬虫项目并导入
在Xfce终端
```
cd Desktop
scrapy startproject vmall_spider
cd vmall_spider
scrapy genspider -t crawl vmall "vmall.com"
```

启动“Pycharm”，启动成功点击“File”->“Open”，选择创建的项目"vmall_spider"

点击“OK”->“This Window”完成项目导入。

## 编写爬虫代码
在项目“vmall_spider”-＞“spiders”下，双击打开“vmall.py”文件，删除原有代码，写入以下代码：

```
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from vmall_spider.items import VmallSpiderItem

class VamllSpider(CrawlSpider):
    name = 'vamll'
    allowed_domains = ['vmall.com']
    start_urls = ['https://sale.vmall.com/huaweizone.html']

    rules = (
        Rule(LinkExtractor(allow=r'.*/product/.*'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        title=response.xpath("//div[@class='product-meta product-global']/h1/text()").get()
        price=response.xpath("//div[@class='product-price-info']/span/text()").get()
        image=response.xpath("//a[@id='product-img']/img/@src").get()
        item=VmallSpiderItem(
            title=title,
            image=image,
        )
        print("="*30)
        print(title)
        print(image)
        print("="*30)
        yield item
```
双击打开“itmes.py”文件，删除原有代码，写入以下代码：
```
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy

class VmallSpiderItem(scrapy.Item):
    title=scrapy.Field()
    image=scrapy.Field()
```

双击打开“pipelines.py”文件，删除原有代码，写入以下代码（使用创建的云数据库RDS的密码、绑定的弹性公网IP替换代码中的相关信息）。
```
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
import pymysql
import os
from urllib import request

class VmallSpiderPipeline:
    def __init__(self):
        dbparams={
            'host':'124.70.15.164', #云数据库弹性公网IP
            'port':3306, #云数据库端口
            'user':'root', #云数据库用户
            'password':'rIDM7g4nl5VxRUpI', #云数据库RDS密码
            'database':'vmall', #数据库名称
            'charset':'utf8'
        }
        self.conn=pymysql.connect(**dbparams)
        self.cursor=self.conn.cursor()
        self._sql=None

        self.path=os.path.join(os.path.dirname(os.path.dirname(__file__)),'images')
        if not os.path.exists(self.path):
            os.mkdir(self.path)

    def process_item(self,item,spider):
        url=item['image']
        image_name=url.split('_')[-1]
        print("--------------------------image_name-----------------------------")
        print(image_name)
        print(url)
        request.urlretrieve(url,os.path.join(self.path,image_name))
        self.cursor.execute(self.sql,(item['title'], item['image']))
        self.conn.commit()
        return item

    @property
    def sql(self):
        if not self._sql:
            self._sql="""
            insert into product(id,title,image) values(null,%s,%s)
            """
            return self._sql
        return self._sql
```

鼠标右键“vmall_spider”（目录第二层）点击“new”->“Python File”创建“start.py”文件，填入名称“start”，点击“OK”，然后写入以下代码。
```
from scrapy import cmdline

cmdline.execute("scrapy crawl vamll".split())
```
双击打开“settings.py”文件，删除原有代码，写入以下代码：
```
# Scrapy settings for vmall_spider project
# python-spider-rds
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://docs.scrapy.org/en/latest/topics/settings.html
#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html
import os

BOT_NAME = 'vmall_spider'

SPIDER_MODULES = ['vmall_spider.spiders']
NEWSPIDER_MODULE = 'vmall_spider.spiders'

# Crawl responsibly by identifying yourself (and your website) on the user-agent
# USER_AGENT = 'vmall_spider (+http://www.yourdomain.com)'

# Obey robots.txt rules
# ROBOTSTXT_OBEY = True
ROBOTSTXT_OBEY = False
```


# 在弹性云服务器ECS上运行爬虫程序
## 安装所需依赖
登录弹性云服务器ECS：

- 切换到【实验操作桌面】已打开的“Xfce终端”，执行以下命令（使用弹性云服务器ECS的公网IP替换命令中的【EIP】），登录云服务器；
```
ssh root@EIP
```
说明: 在华为云控制点展开左侧菜单栏，点击“服务列表”->“计算”->“弹性云服务器ECS”进入管理列表，可查看复制名称为“python-spider”的ECS服务器的弹性公网IP。

- 接受秘钥输入“yes”，回车；
- 输入密码：使用预置环境信息中云服务器名称为python-spider的用户密码（输入密码时，命令行窗口不会显示密码，输完之后直接键入回车）。

依次执行以下命令安装所需依赖。
```
yum -y groupinstall "Development tools"
```
```
yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel
```
依次执行以下命令安装依赖。
```
yum install gcc libffi-devel python-devel openssl-devel -y
```
```
yum install libxslt-devel -y
```
执行以下命令升级pip。
```
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip
```
依次执行以下命令安装pip依赖。
```
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple scrapy
```
```
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple selenium
```
```
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pymysql
```
```
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pillow
```

## 上传爬虫项目并运行
执行以下命令退出弹性云服务器ECS登录状态。
```
exit
```
执行以下命令（使用弹性云服务器ECS的弹性公网IP替换命令中的EIP）上传项目到弹性云服务器ECS，提示输入密码为：使用预置环境信息中云服务器名称为python-spider的用户密码

```
cd /home/user/Desktop && scp -r ./vmall_spider root@EIP:/root
```
完成上传操作

重新登录弹性云服务器ECS，登录成功后执行以下命令，可查看到上传的项目。
```
ls
```

执行以下命令启动爬虫项目，运行片刻（约30秒），按“Ctrl+Z”键停止运行程序。
```
cd /root/vmall_spider/vmall_spider/ && python3 start.py
```
## 查看爬取数据
切换至【实验操作桌面】浏览器已登录云数据库RDS页面，点击“对象列表”->“打开表”

可看到已爬取的数据


# 存储爬取图片至对象存储服务OBS
## 创建对象存储服务OBS

## 安装对象存储服务OBS客户端
在弹性云服务器ECS上安装对象存储服务OBS客户端。

切换至已登录弹性云服务器ECS的命令行界面，执行以下命令下载“obsutil”工具。
```
cd ~ && wget https://sandbox-experiment-resource-north-4.obs.cn-north-4.myhuaweicloud.com/python-spider-rds/obsutil_linux_amd64.tar.gz
```
执行以下命令解压“obsutil”工具。
```
tar zvxf obsutil_linux_amd64.tar.gz
```
解压成功

切换至实验浏览器华为云控制台页面，在右上角账号名下拉菜单中选择“我的凭证”-＞“访问秘钥”，进入创建管理访问密钥（AK/SK）的界面。

删除原有的访问密钥，再进行新密钥的创建。点击【新增访问密钥】，在弹框中点击【确定】即可完成新增，然后点击“立即下载”选择“保存文件”->“确定”， 将密钥保存至浏览器默认文件保存路径/home/user/Downloads/，妥善保存系统自动下载的“credentials.csv”文件中的AK（Access Key Id）和SK（Secret Access Key）以备后面操作使用。

AK和SK查看方式：

切换至【实验操作桌面】双击图标“Xfce 终端”新打开一个命令行界面，输入以下命令，切换到目录（/home/user/Downloads/）下。
```
cd /home/user/Downloads/
```
输入以下命令，即可查看AK和SK内容：
```
vi credentials.csv
```
```
User Name,Access Key Id,Secret Access Key
"Sandbox-user",QLSEDHXJOPVVMNJ9RYAM,7XGRjAjjKKAjgzp0RIAwSzCwZgyHhHGwwbpEttY3
```
切换到已登录弹性云服务器ECS的命令行界面，执行以下命令（使用上面所示AK、SK替换命令中的“your_ak”、“your_sk”），初始化“obsutil”。
```
./obsutil_linux_amd64_5.2.5/obsutil config -i=your_ak -k=your_sk -e=https://obs.cn-north-4.myhuaweicloud.com
```
初始化成功后执行以下命令查看“obsutil”连通性。
```
./obsutil_linux_amd64_5.2.5/obsutil ls -s
```
## 上传爬取图片至对象存储服务OBS并查看
继续执行以下命令【使用创建的对象存储服务OBS的名称（如：obs-spider）替换命令中的“your_bucket_name”】，上传爬取到的图片。
```
/root/obsutil_linux_amd64_5.2.5/obsutil cp /root/vmall_spider/images obs://your_bucket_name -f -r -vmd5 -flat -u
```
找到创建的对象存储服务OBS，点击名称打开详情页，点击左侧栏“对象”可看到上传保存的爬取图片
