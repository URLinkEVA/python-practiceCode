上一章介绍了神经网络的概要，重点关注了神经网络在识别时进行的处理
[链接](https://blog.csdn.net/qq_45618521/article/details/122635957)
这一章的“学习”是指从训练数据中自动获取最优权重参数的过程，为了能够学习，导入损失函数这一指标。我们学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。
# 从数据中学习
神经网络的特征就是可以从数据中学习，指的是由数据自动决定权重参数的值。
之前感知机对照真值表人工设定了3个参数值，而实际的神经网络参数的数量成千上万甚至过亿，想用人工决定参数的值就不可能了。这一章就是介绍神经网络的学习，即利用数据决定参数值的方法。
## 数据驱动
数据是机器学习的核心。机器学习的方法是极力避免人为介入，尝试从收集到的数据中发现答案。例如设计一个识别5的程序，人可以很简单的识别出，但却很难明确说出是基于何种规律而识别出5。可以通过数据来解决这个问题：
### 一种方案
先从图像中提取**特征量**，再用机器学习技术学习这些特征量的模式。这里的特征量是指可以从输入数据中准确提取本质数据的转换器。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM，KNN等分类器进行学习。

**注意点**：将图像转换为向量时使用的特征量仍然由人设计。对于不同的问题，必须设计专门的特征量，才能得到好的结果。
![在这里插入图片描述](https://img-blog.csdnimg.cn/bb2d349d37564e47a60529a437d0d887.png)
如图神经网络直接学习图像本身，第二个路线中特征量仍是由人工设计的。神经网络的优点是对所有的问题都可以用同样的流程来解决，通过不断学习所提供的数据，尝试发现待求解的问题的模式。
## 训练数据和测试数据
机器学习中，一般将数据分为**训练数据**和**测试数据**两部分来进行学习和实验等。首先使用训练数据学习，寻找最优的参数，然后使用测试数据评价训练得到的模型的实际能力。之所以这样做就是追求模型的泛化能力，此外训练数据也可以称为监督数据。

获得泛化能力是机器学习的最终目标。仅通过一个数据集去学习和评价参数，会导致对某个数据集过度拟合的**过拟合**状态。
# 损失函数
神经网络的学习通过某个指标表示现在的状态，然后以这个指标为基准，寻找最优权重参数。所用的指标称为**损失函数**(loss function)。这个损失函数可以使用任意函数，一般用均方误差和交叉熵误差
## 均方误差
可用作损失函数的函数有很多，其中最有名的是均方误差(mean squared error)
![在这里插入图片描述](https://img-blog.csdnimg.cn/4f107167cdb643d2a0ec6ff7185c06d1.png)
$y_k$表示神经网络的输出，$t_k$表示监督数据，$k$表示数据的维数
均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方，再求总和。
```python
def mean_squared_error(y,t):
    return 0.5 * np.sum((y-t)**2)
```
实际计算
```python
import numpy as np

def mean_squared_error(y,t):
    return 0.5 * np.sum((y-t)**2)

y = [0,0,1,0,0,0,0]
t = [0.1,0.05,0.6,0.0,0.05,0.1,0.0]

print(mean_squared_error(np.array(y),np.array(t)))
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/3b5fd962b05940c1a31f9305f92e8b40.png)

## 交叉熵误差
交叉熵误差(cross entropy error)也经常被用作损失函数
log表示以e为底的自然对数，$y_k$是神经网络的输出，$t_k$是正确解标签（~~我个人觉得$logy_k$写成$lny_k$更眼熟一点~~ ）
![在这里插入图片描述](https://img-blog.csdnimg.cn/59bf836c90a142eba9d09a90ce5fa3a1.png)
```python
def cross_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```
计算np.log时，加上了一个微小值delta，防止负无限大的发生
简单计算：

```python
def cross_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))

y = [0,0,1,0,0,0,0]
t = [0.1,0.05,0.6,0.0,0.05,0.1,0.0]

print(cross_entropy_error(np.array(y),np.array(t)))
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/ab971f7b3ed2465ebcafa0e1d4c2df09.png)

## mini-batch学习
机器学习使用训练数据进行学习，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。所以计算损失函数时必须将所有的训练数据作为对象。

之前的例子都是针对单个数据的损失函数，若要所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成以下形式
![在这里插入图片描述](https://img-blog.csdnimg.cn/892c18b9353141a885d8ff88ad3b3d44.png)
$t_nk$表示第n个数据的第k个元素的值，式子看起来有点复杂，其实只是把求单个数据的损失函数的式扩大到了N份数据，不过最后还是要除以N进行正规化。这样就能得到单个数据的“平均损失函数”。通过这样的平均化，可以获得和训练数据的数量无关的统一指标。

另外，MNIST数据集的训练数据有60000个，如果以全部数据为对象求损失函数的和，计算会花费很多时间，遇上几千万以上的数据量更难求。因此需要我们从全部数据中选出一部分，作为全体数据的“近似”。神经网络的学习就是从训练数据中选出一部分数据（称为mini-batch，小批量），然后对每个mini-batch进行学习。例如从60000个训练数据中随机选择100笔，再用这100笔数据进行学习。这种学习方式称为**mini-batch学习**

读入MNIST数据集的代码：
```python
import sys,os
sys.path.append(os.pardir)
import numpy as np
from dateset.mnist import load_mnist

(x_train,t_train),(x_test,t_test) = \
    load_mnist(normalize = True,one_hot_label = True)

print(x_train.shape)  # (60000. 784)
print(t_train.shape)  # (60000, 10)             
```
通过设定参数one_hot_label = True，可以得到one_hot表示（仅正确解标签为1，其余为0的数据结构），代码中的两行注释表示训练数据有60000个，输入数据是784维（28×28）的图像数据，监督数据是10维的数据。

从这个训练数据中随机抽取10笔数据，可以使用numpy的np.random.choice()，写成如下形式：

```python
trian_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(trian_size,batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```
使用np.random.choice()可以从指定的数字中随机选择想要的数字，可以得到一个包含被选数据的索引的数组，然后只需指定这些随机选出的索引，取出mini-batch，然后使用这个mini-batch计算损失函数即可。


# 数值微分
梯度法使用梯度的信息决定前进的方向
## 导数
导数表示的是某个瞬间的变化量，定义成以下的式子：
![在这里插入图片描述](https://img-blog.csdnimg.cn/fcda9f2e2bd84d5e8899d782867855fe.png)
左边符号表示f(x)关于x的导数，即f(x)相对于x的变化程度

```python
# 不好的示例实现
def numerical_diff(f,x):
    h = 10e-50
    return (f(x+h)-f(x))/h
```
这个函数有两个参数，函数 f 和传给函数 f 的参数 x ，这段代码两处地方需要改进，想把尽可能小的值赋给 h，所以采用了10e-50这个微小值，但会产生**舍入误差**。（因省略小数的精细部分的数值而造成最终的计算结果上的误差）
![在这里插入图片描述](https://img-blog.csdnimg.cn/ba209ddb24374920a62a5af4e25c4b0c.png)
如果用32位的浮点数来表示1e-50，就会变成0.0，无法正确表示，使用过小的值会造成计算上的问题，可以将h改成10^-4

第二个要改进的地方与 f 的差分有关，真的导数对应函数在x处的斜率（切线），但在代码实现上是（x+h）和 x 之间的斜率，两者得到的导数值在严格意义上并不一致，这个差异的出现是因为 h 不可能无限接近0

所以计算 f 在（x+h）和（x-h）之间的差分，也成为**中心差分**

```python
def numerical_diff(f,x):
    h = 1e-4 # 0.0001
    return (f(x+h)-f(x-h)) / (2*h)
```
## 数值微分的例子
尝试用上述的数值微分对简单函数进行求导
![在这里插入图片描述](https://img-blog.csdnimg.cn/fcdd5676851f4f52b8d810785bd21e7e.png)
```python
def func1(x):
    return 0.01*x**2 + 0.1*x
```
绘制图像

```python
import numpy as np
import matplotlib.pyplot as plt

x= np.arange(0.0,20.0,0.1)  # 0到20,0.1为单位
y= func1(x)
plt.xlabel("x")
plt.ylabel("f(x)")
plt.plot(x,y)
plt.show()
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/2921156f11d44793a84539fa43c87068.png)
来计算一下这个函数在x=5和x=10的导数

```python
print(numerical_diff(func1,5))
print(numerical_diff(func1,10))
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/5ab5f3abf627426ea00d1509b7a4a9df.png)
得出的结果与真的导数误差极为小，基本上可以看作相等

画出x=5的切线，完整代码如下：

```python
def numerical_diff(f,x):
    h = 1e-4 # 0.0001
    return (f(x+h)-f(x-h)) / (2*h)

def func1(x):
    return 0.01*x**2 + 0.1*x
    

import numpy as np
import matplotlib.pyplot as plt

x= np.arange(0.0,20.0,0.1)  # 0到20,0.1为单位
y= func1(x)
plt.xlabel("x")
plt.ylabel("f(x)")


def tangent_line(f, x):
    d = numerical_diff(f, x)
    print(d)
    y = f(x) - d*x
    return lambda t: d*t + y

tf = tangent_line(func1, 5)
y2 = tf(x)

plt.plot(x,y)
plt.plot(x,y2)
plt.show()
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/16b196c0f3d84cc8a6751f7ded7b3a1e.png)

## 偏导数
![在这里插入图片描述](https://img-blog.csdnimg.cn/22b431c898594eb6b49f68729196dabb.png)
具有两个变量的函数

```python
def func2(x):
    return x[0]**2 + x[1]**2
```
画一下这个函数的图像
![在这里插入图片描述](https://img-blog.csdnimg.cn/4e1dafdad1a140f7b753ee8a432b0168.png)
来求该函数的导数，但是式中有两个变量，所以要区分对$x_0$还是$x_1$进行求导数，此外将有多个变量的函数的导数称为**偏导数**，用数学式可以写成
![在这里插入图片描述](https://img-blog.csdnimg.cn/ed3429b78107454786aea5a5bcef99ec.png)
### 求解两个关于偏导数的例子
#### 题目一
![在这里插入图片描述](https://img-blog.csdnimg.cn/6825cbe1013449ee883de3341b5aa7f6.png)
```python
def numerical_diff(f,x):
    h = 1e-4 # 0.0001
    return (f(x+h)-f(x-h)) / (2*h)

def func_t1(x0):
    return x0*x0 + 4.0**2.0

print(numerical_diff(func_t1,3.0))
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/a84e1eb268dd44cc9e689d9da1cfa61e.png)
#### 题目二
![在这里插入图片描述](https://img-blog.csdnimg.cn/f5309d5bf86149cd8037e256c526aff1.png)

```python
def func_t2(x1):
    return 3.0**2.0 + x1*x1  

print(numerical_diff(func_t2,4.0))
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/eb7832b106c747feae0aac7df0438b1d.png)
与解析解的导数基本一致，偏导数与单变量的导数一样，都是求某个某个地方的斜率。不过偏导数需要将多个变量中的某一变量定为目标变量，并将其他变量固定为某个值。
