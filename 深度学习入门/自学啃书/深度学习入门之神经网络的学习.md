上一章介绍了神经网络的概要，重点关注了神经网络在识别时进行的处理
[链接](https://blog.csdn.net/qq_45618521/article/details/122635957)
这一章的“学习”是指从训练数据中自动获取最优权重参数的过程，为了能够学习，导入损失函数这一指标。我们学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。
# 从数据中学习
神经网络的特征就是可以从数据中学习，指的是由数据自动决定权重参数的值。
之前感知机对照真值表人工设定了3个参数值，而实际的神经网络参数的数量成千上万甚至过亿，想用人工决定参数的值就不可能了。这一章就是介绍神经网络的学习，即利用数据决定参数值的方法。
## 数据驱动
数据是机器学习的核心。机器学习的方法是极力避免人为介入，尝试从收集到的数据中发现答案。例如设计一个识别5的程序，人可以很简单的识别出，但却很难明确说出是基于何种规律而识别出5。可以通过数据来解决这个问题：
### 一种方案
先从图像中提取**特征量**，再用机器学习技术学习这些特征量的模式。这里的特征量是指可以从输入数据中准确提取本质数据的转换器。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM，KNN等分类器进行学习。

**注意点**：将图像转换为向量时使用的特征量仍然由人设计。对于不同的问题，必须设计专门的特征量，才能得到好的结果。
![在这里插入图片描述](https://img-blog.csdnimg.cn/bb2d349d37564e47a60529a437d0d887.png)
如图神经网络直接学习图像本身，第二个路线中特征量仍是由人工设计的。神经网络的优点是对所有的问题都可以用同样的流程来解决，通过不断学习所提供的数据，尝试发现待求解的问题的模式。
## 训练数据和测试数据
机器学习中，一般将数据分为**训练数据**和**测试数据**两部分来进行学习和实验等。首先使用训练数据学习，寻找最优的参数，然后使用测试数据评价训练得到的模型的实际能力。之所以这样做就是追求模型的泛化能力，此外训练数据也可以称为监督数据。

获得泛化能力是机器学习的最终目标。仅通过一个数据集去学习和评价参数，会导致对某个数据集过度拟合的**过拟合**状态。
# 损失函数
神经网络的学习通过某个指标表示现在的状态，然后以这个指标为基准，寻找最优权重参数。所用的指标称为**损失函数**(loss function)。这个损失函数可以使用任意函数，一般用均方误差和交叉熵误差
## 均方误差
可用作损失函数的函数有很多，其中最有名的是均方误差(mean squared error)
![在这里插入图片描述](https://img-blog.csdnimg.cn/4f107167cdb643d2a0ec6ff7185c06d1.png)
$y_k$表示神经网络的输出，$t_k$表示监督数据，$k$表示数据的维数
均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方，再求总和。
```python
def mean_squared_error(y,t):
    return 0.5 * np.sum((y-t)**2)
```
实际计算
```python
import numpy as np

def mean_squared_error(y,t):
    return 0.5 * np.sum((y-t)**2)

y = [0,0,1,0,0,0,0]
t = [0.1,0.05,0.6,0.0,0.05,0.1,0.0]

print(mean_squared_error(np.array(y),np.array(t)))
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/3b5fd962b05940c1a31f9305f92e8b40.png)

## 交叉熵误差
交叉熵误差(cross entropy error)也经常被用作损失函数
log表示以e为底的自然对数，$y_k$是神经网络的输出，$t_k$是正确解标签（~~我个人觉得$logy_k$写成$lny_k$更眼熟一点~~ ）
![在这里插入图片描述](https://img-blog.csdnimg.cn/59bf836c90a142eba9d09a90ce5fa3a1.png)
```python
def cross_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```
计算np.log时，加上了一个微小值delta，防止负无限大的发生
简单计算：

```python
def cross_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))

y = [0,0,1,0,0,0,0]
t = [0.1,0.05,0.6,0.0,0.05,0.1,0.0]

print(cross_entropy_error(np.array(y),np.array(t)))
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/ab971f7b3ed2465ebcafa0e1d4c2df09.png)

## mini-batch学习
机器学习使用训练数据进行学习，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。所以计算损失函数时必须将所有的训练数据作为对象。

之前的例子都是针对单个数据的损失函数，若要所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成以下形式
![在这里插入图片描述](https://img-blog.csdnimg.cn/892c18b9353141a885d8ff88ad3b3d44.png)
$t_nk$表示第n个数据的第k个元素的值，式子看起来有点复杂，其实只是把求单个数据的损失函数的式扩大到了N份数据，不过最后还是要除以N进行正规化。这样就能得到单个数据的“平均损失函数”。通过这样的平均化，可以获得和训练数据的数量无关的统一指标。

另外，MNIST数据集的训练数据有60000个，如果以全部数据为对象求损失函数的和，计算会花费很多时间，遇上几千万以上的数据量更难求。因此需要我们从全部数据中选出一部分，作为全体数据的“近似”。神经网络的学习就是从训练数据中选出一部分数据（称为mini-batch，小批量），然后对每个mini-batch进行学习。例如从60000个训练数据中随机选择100笔，再用这100笔数据进行学习。这种学习方式称为**mini-batch学习**

读入MNIST数据集的代码：
```python
import sys,os
sys.path.append(os.pardir)
import numpy as np
from dateset.mnist import load_mnist

(x_train,t_train),(x_test,t_test) = \
    load_mnist(normalize = True,one_hot_label = True)

print(x_train.shape)  # (60000. 784)
print(t_train.shape)  # (60000, 10)             
```
通过设定参数one_hot_label = True，可以得到one_hot表示（仅正确解标签为1，其余为0的数据结构），代码中的两行注释表示训练数据有60000个，输入数据是784维（28×28）的图像数据，监督数据是10维的数据。

从这个训练数据中随机抽取10笔数据，可以使用numpy的np.random.choice()，写成如下形式：

```python
trian_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(trian_size,batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```
使用np.random.choice()可以从指定的数字中随机选择想要的数字，可以得到一个包含被选数据的索引的数组，然后只需指定这些随机选出的索引，取出mini-batch，然后使用这个mini-batch计算损失函数即可。
